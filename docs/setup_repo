Publish a package with specialized, robust, spatial code. With an effort to start a standardized repository for useful spatial tools, based off simple statistics.

Package represents a “for computational biologists, by computational biologists” mentality, with the perspective of industry, scalability with large spatial datasets, and ease of deployment that is needed in this field. 

when writing functions, have clear docstrings and include references where possible. 

Code to include; (see @docs/) ontology conversion, univariate cut off, multivariate cut off, original nanostring neighborhoods code, make domains code, compute domain distance code, spatial NMF code, morans I and Lees L implementation with HH LL HL LH, manual diffusion map and pseudo time code, etc. I want to have both R and python implementations, when possible. 

Goal is to seamlessly fit into scanpy and squidpy workflows. 

output from the package will include html markdown/vingettes for each function or class of fucntions. it wil linclude:
- images for intuition of what is happening with the data (will be provided manually)
- Recommendations for cell typing. Recommendations for QC.
- Make standard definitions. compare “neighborhood” vs “domain” or “niches” or “community” and highlight tissue decomposition. 
- ultimatley it will serve as a 'txtbook' Could make a short white paper demonstrating tissue decomposition, some basic biology, and package functionality using publicly available Xenium and CosMx data

In the REAMDE I want to:
Highlight the package is nothing novel, but simply wrappers and centralization of useful statistical tools for spatial bio. Outline all intuitive steps, and packages used. 
Highlight packages "weve liked" celltypist. We will include code for custom model training from cell x gene downloads. Show custom model benchmark. Hieratype on cosmx, show celltypist vs hiratype benchmark

Recommended README Philosophy
To attract collaborators and satisfy your "by computational biologists" goal, your README should lead with:
The Problem: We spend too much time re-implementing basic spatial statistics. The Solution: A thin, robust wrapper around standard libraries to ensure a Python user and an R user get the exact same result for the same biological question. The Goal: Not novel math—just better, standardized engineering for the community.

I want the readme and github repo to look like a professional package

we may want two repos or branches: 1 for package deployment, 1 for dev (celltypist and models, etc)

we may develop snakemake pipelines?

I want the package to be able to be installed via pip/python in a python env or otherwise. we will need to outline python and R requirements.

Cross-language: rpy2 for calling R from Python, or separate R scripts with anndata/h5ad interchange
Data formats: AnnData (.h5ad) as primary object, platform-specific raw inputs (CosMx, Xenium, Visium), JSON metadata

Coding Conventions
Use snake_case naming for functions, variables, and files (not . separators)
Each function should do one specific thing with clear input and output
All functions must include:
Docstrings (Python) or roxygen2 documentation (R)
Type hints (Python) or parameter typing (R)
Input validation
Descriptive error messages
Keep functions platform-agnostic where possible; use platform-specific wrappers when needed


Parallelization strategy
Default: Sequential sample processing (memory-safe for large spatial datasets)
Within-function: Use numba/parallel operations where available (scanpy defaults)
Optional: Parallel samples via multiprocessing or job scheduler if memory permits
R integration
For analyses requiring R/Seurat (e.g., certain annotation methods):

rpy2 bridge: Call R functions directly from Python via R/r_bridge.py
h5ad interchange: Save AnnData, load in R with anndata package, process, save back
# Example R bridge usage
from R.r_bridge import run_seurat_analysis
adata = run_seurat_analysis(adata, method="sctransform")

add a metadata.json file for tracking of adata operations for each function
add clear logging for each function


Notes for Claude
When creating functions, always ask which platform(s) it should support
Prefer vectorized operations over loops
Use existing packages (scanpy, squidpy, spatialdata) as building blocks; Seurat for R-specific needs
Always update metadata.json after processing steps
Cache intermediate results to .cache/ directory as .h5ad files
When running scripts or installing packages, use the spatialcore mamba environment
Follow scanpy conventions: modify in-place by default, offer copy=True option
Store computed results in adata.obs, adata.var, adata.obsm, adata.uns as appropriate

a proper testing framework for code: something liek

1️⃣ Concept

A YAML-based test definition usually has three parts:

Function info – name, arguments, language

Inputs / test cases – what inputs to pass

Expected outputs – what the function should return

Example YAML structure:

tests:
  - language: python
    function: add_numbers
    inputs:
      - [2, 3]
      - [10, -5]
    expected_outputs:
      - 5
      - 5

  - language: R
    function: multiply_numbers
    inputs:
      - [2, 3]
      - [4, 5]
    expected_outputs:
      - 6
      - 20

2️⃣ How to run tests

You can write a Python script that reads the YAML, executes the functions, and compares results:

import yaml

# Example Python functions
def add_numbers(a, b):
    return a + b

# Load YAML
with open("tests.yaml") as f:
    data = yaml.safe_load(f)

for test in data["tests"]:
    if test["language"] == "python":
        func = globals()[test["function"]]
        for inp, expected in zip(test["inputs"], test["expected_outputs"]):
            result = func(*inp)
            assert result == expected, f"{test['function']}({inp}) = {result}, expected {expected}"
print("All Python tests passed!")

3️⃣ R functions

Similarly, in R, you could parse the YAML and run tests with testthat:

library(yaml)
library(testthat)

# Example R function
multiply_numbers <- function(a, b) {
  return(a * b)
}

tests <- yaml::read_yaml("tests.yaml")

for(test in tests$tests) {
  if(test$language == "R") {
    func <- get(test$function)
    for(i in seq_along(test$inputs)) {
      input <- test$inputs[[i]]
      expected <- test$expected_outputs[[i]]
      result <- do.call(func, input)
      test_that(paste(test$function, input), {
        expect_equal(result, expected)
      })
    }
  }
}

4️⃣ Advantages

Single source of truth for tests, easy to edit or extend

Works with multiple languages (Python, R, etc.)

Can be integrated with Claude/Gemini for auto-generating tests based on code

Fits nicely in CI/CD pipelines or automated multi-agent workflows